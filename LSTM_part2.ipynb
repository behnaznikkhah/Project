{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUbKFqMLr1FHKNqYs3SOQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/behnaznikkhah/Project/blob/master/LSTM_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8W-5DZUg9h5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from numpy import concatenate\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import LeakyReLU\n",
        "import numpy\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq7N7d1jhBOm",
        "outputId": "b306de2d-7e3c-4643-f82f-7205aca22b29"
      },
      "source": [
        "#load datasets\n",
        "file_path_covid= os.path.abspath(\"\")\n",
        "file_path_market = os.path.abspath(\"\")\n",
        "\n",
        "Covid = pd.read_csv(os.path.join(file_path_covid, 'Covid.csv'))\n",
        "Market = pd.read_csv(os.path.join(file_path_market, 'Market.csv'))\n",
        "\n",
        "#make a copy of covid dataset \n",
        "CovidData = Covid\n",
        "\n",
        "#Impute missing values with the next valid observation \n",
        "CovidData=CovidData.fillna(method='bfill')\n",
        "\n",
        "#cumalative cases and deaths\n",
        "CovidData=CovidData[CovidData.Country.eq('World')]\n",
        "\n",
        "#make a copy of markets data\n",
        "MarketData = Market\n",
        "\n",
        "#get the name of unique countries\n",
        "countrynames = MarketData.Country.unique()\n",
        "\n",
        "#ask the user to enter a name of country\n",
        "country = input(\"Please enter a country name: \")\n",
        "\n",
        "#save the given country's data to work on it \n",
        "if country in  countrynames:\n",
        "    MarketData=MarketData[MarketData.Country.eq(country)]\n",
        "else:\n",
        "    print(\"Please enter a country name, list of countries are:\",countrynames)\n",
        "\n",
        "#drop all columns except Date and Price \n",
        "MarketData = MarketData.drop(['Volume','Low','Open','Change','High','Country'], 1)\n",
        "\n",
        "#create a differenced column named cases_diff, shows the number of new active cases in each day \n",
        "CovidData['cases_diff'] = CovidData['ActiveCases'].diff()\n",
        "\n",
        "#create a differenced column named deaths_diff, shows the number of deaths in each day \n",
        "CovidData['deaths_diff'] = CovidData['Deaths'].diff()\n",
        "\n",
        "#drop columns\n",
        "CovidData = CovidData.drop(['ActiveCases','Deaths','Country'], 1)\n",
        "\n",
        "#join two dataset\n",
        "result = pd.merge(CovidData, MarketData, how='inner', on=['Date'])\n",
        "\n",
        "#change the order of columns\n",
        "result=result[['Date','Price','cases_diff','deaths_diff']]\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter a country name: Canada\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNkUW0jOhtM-"
      },
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdizSE3wOqwi"
      },
      "source": [
        "# inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "\tnew_row = [x for x in X] + [value]\n",
        "\tarray = numpy.array(new_row)\n",
        "\tarray = array.reshape(1, len(array))\n",
        "\tinverted = scaler.inverse_transform(array)\n",
        "\treturn inverted[0, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssLn7f7friO4"
      },
      "source": [
        "# load dataset\n",
        "dataset = result\n",
        "\n",
        "#set Date column as Index\n",
        "dataset=dataset.set_index('Date')\n",
        "\n",
        "# ensure all data is float\n",
        "values = dataset.values\n",
        "values = values.astype('float32')\n",
        "\n",
        "# frame as supervised learning\n",
        "reframed = series_to_supervised(values, 30,1)\n",
        "\n",
        "#exclude the previous value of Index price\n",
        "lst = [x for x in range(0, 90) if x % 3 == 0]\n",
        "reframed.drop(reframed.columns[lst], axis=1, inplace=True)\n",
        "\n",
        "# drop columns we don't want to predict\n",
        "reframed.drop(reframed.columns[[61,62]], axis=1, inplace=True)\n",
        "\n",
        "# normalize features\n",
        "values = reframed.values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRU_T548sVmd",
        "outputId": "0f347518-f399-4d61-f14d-064e01d7ef35"
      },
      "source": [
        "# split into train and test sets\n",
        "n_train = 150\n",
        "train = scaled[:n_train, :]\n",
        "test = scaled[n_train:, :]\n",
        "\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]\n",
        "\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape,scaled.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 1, 60) (150,) (43, 1, 60) (43,) (193, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDRI7lMEWljf"
      },
      "source": [
        "#design a LSTM network\n",
        "def lstm_model (activation,optimizer):\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "  np.random.seed(0)\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(50,activation=activation, return_sequences=True,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "  model.add(LSTM(10,activation=activation))\n",
        "  model.add(Dense(1,activation=activation))\n",
        "  model.compile(loss='mse', optimizer = optimizer,metrics=[\"mae\"])\n",
        "  return model\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6odbO3wYDZM"
      },
      "source": [
        "#fit LSTM network for each country\n",
        "if country=='United Arab Emirates':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer =tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country=='Brazil':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 200,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country=='France':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer =tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 80,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country == 'Germany':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 200,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country=='Hong Kong':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer =tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 60,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country == 'Indonesia':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 150,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country == 'South Korea':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 150,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='United States':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 150,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='India':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 80,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='Japan':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer =tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='Asutralia':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer ='adam')\n",
        "  history=model.fit(train_X, train_y, epochs = 30,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='Canada':\n",
        "  model=lstm_model(activation='sigmoid',optimizer ='adam')\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='China':\n",
        "  model=lstm_model(activation='sigmoid',optimizer =tf.keras.optimizers.Adam(learning_rate=5e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 150,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "elif country =='South Africa':\n",
        "  model=lstm_model(activation=tf.keras.layers.LeakyReLU( ),optimizer =tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-SesrEkS8b_"
      },
      "source": [
        "\n",
        "#design a RNN network\n",
        "def RNN_model (activation,optimizer):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(50, return_sequences=True,input_shape = (train_X.shape[1], train_X.shape[2]), activation = activation))\n",
        "    model.add(tf.keras.layers.SimpleRNN(10, activation = activation))\n",
        "    model.add(Dense(1, activation = activation))\n",
        "\n",
        "    # Compile your model with your optimizer, loss, and metrics\n",
        "    model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mae'])\n",
        "if country =='Canada':\n",
        "  model=RNN_model(activation='sigmoid',optimizer ='adam')\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2ZVp9JwTrex"
      },
      "source": [
        "#design a MLP network\n",
        "def MLP_model (activation,optimizer):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50,input_shape = (train_X.shape[1], train_X.shape[2]), activation=activation))\n",
        "    model.add(Dense(10, activation=activation))\n",
        "    model.add(Dense(1,activation=activation))\n",
        "\n",
        "    # Compile the model with the optimizer, loss, and metrics\n",
        "    model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mae'])\n",
        "if country =='Canada':\n",
        "  model=MLP_model(activation='sigmoid',optimizer ='adam')\n",
        "  history=model.fit(train_X, train_y, epochs = 100,batch_size=1, validation_data=(test_X, test_y), verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCCOom2cjiU5"
      },
      "source": [
        "# forecast the entire training dataset to build up state for forecasting\n",
        "model.predict(train_X, batch_size=1)\n",
        "\n",
        "#make a one-step forecast\n",
        "forecast = []\n",
        "for time in range(len(scaled)):\n",
        "        X=scaled[time:time+1][:,:-1]\n",
        "        X = X.reshape((1, 1, 60))\n",
        "        yhat = model.predict(X)\n",
        "        new_X=scaled[time, 0:-1]\n",
        "        yhat_invert = invert_scale(scaler, new_X, yhat[0,0])\n",
        "        forecast.append(yhat_invert)\n",
        "\n",
        "plt.plot(forecast,label='forecast')\n",
        "plt.plot(values[:,:][:,-1],label='actual')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaoPWw3JjNZS"
      },
      "source": [
        "# Evaluate the model \n",
        "print(\"Final loss value:\",model.evaluate(test_X, test_y))\n",
        "\n",
        "# plot history\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHgEYn77jqP5"
      },
      "source": [
        "# calculate MAE\n",
        "print(tf.keras.metrics.mean_absolute_error(values[:,-1], forecast).numpy())\n",
        "\n",
        "# calculate RMSE\n",
        "rmse = sqrt(mean_squared_error(values[:,-1], forecast))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}