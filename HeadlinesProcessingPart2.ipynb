{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeadlinesProcessingPart2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPl+FWhekSthGFO1J3KaZvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/behnaznikkhah/Project/blob/master/HeadlinesProcessingPart2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh3Bh4bFu1u4",
        "outputId": "5f7cdcb1-a52b-4109-f16b-059f94e77353"
      },
      "source": [
        "# CovidHeadlinesAnalysisPart2.py\n",
        "\n",
        "# Sai Madhuri Yerramsetti\n",
        "# November 10, 2020\n",
        "# Student Number: 0677671\n",
        "\n",
        "# import required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Flatten, Dense, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta,Adam,RMSprop\n",
        "from keras.utils import np_utils\n",
        "from keras import Sequential, optimizers, regularizers\n",
        "# This is required if nltk related words corpus is not downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# To disable warnings which arise from chained assignment\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# This is the function which combines all the headlines for each date\n",
        "def combine_news_strings(news_file_path):\n",
        "\n",
        "    #initialize the variables\n",
        "    merged_news = []\n",
        "\n",
        "    # Read the file\n",
        "    corona_news = pd.read_csv(news_file_path)\n",
        "\n",
        "    # Check for the datatypes information\n",
        "    print(\"Info of the corona news data: \\n\", corona_news.info(verbose=True))\n",
        "\n",
        "    # Convert the Date column to datetime type, sort the values and check for any null values\n",
        "    corona_news['Date'] = pd.to_datetime(corona_news['Date'], format='%Y%m%d')\n",
        "    corona_news.sort_values(by='Date')\n",
        "    print(\"Corona news data before combining strings: \\n\", corona_news.head(5))\n",
        "    print(\"Number of missing values: \", corona_news.isnull().sum())\n",
        "\n",
        "    # Join the news strings datewise, drop the duplicate date rows and reset the index\n",
        "    corona_news['News'] = corona_news.groupby(['Date'])['News'].transform(lambda x : ' '.join(x))   \n",
        "    corona_news = corona_news.drop_duplicates(subset = ['Date'])\n",
        "    corona_news = corona_news.reset_index(drop=True)\n",
        "\n",
        "    print(\"Corona news data after combining headlines strings: \\n\", corona_news.head(5))\n",
        "\n",
        "    # Return the new data\n",
        "    return corona_news\n",
        "    \n",
        "# Function to search for the abbreviations in headlines and return them \n",
        "def search_abbreviation(text):\n",
        "    abbreviations = []\n",
        "    for i in re.finditer(r\"([A-Z]){2,}|([A-Za-z]+| )([A-Za-z]\\.){2,}\", text):\n",
        "        abbreviations.append(i.group())    \n",
        "    return pd.Series(abbreviations).unique()\n",
        "\n",
        "# This function expands the abbrevaitions in the text, tokenize, remove special characters and stop words \n",
        "def tokenize_and_preprocess_text(text):\n",
        "    # Added extra stop words which are present in most of the headlines to nltk stop words corpus \n",
        "    extra_stop_words = ['news', 'html', 'php', 'today', 'yesterday', 'aspx']\n",
        "    stop_words_final = stopwords.words('english') + extra_stop_words   \n",
        "    \n",
        "    # Expand all the abbreviations and remove some irrelevant strings from the text\n",
        "    text = re.sub(r\" WHO \", \" world health organization \", text)\n",
        "    text = re.sub(r\" IMF \", \" international monetary fund \", text)\n",
        "    text = re.sub(r\" ATM \", \" Automated Teller Machine \", text)\n",
        "    text = re.sub(r\" CIA \", \" Central Intelligence Agency \", text)\n",
        "    text = re.sub(r\" L.A. \", \" los angeles \", text)\n",
        "    text = re.sub(r\" EU \", \" Europian Union \", text)\n",
        "    text = re.sub(r\" LGBT \", \" minority \", text)\n",
        "    text = re.sub(r\" FBI \", \" Federal Bureau of Investigation \", text)\n",
        "    text = re.sub(r\" HIV \", \" Human immunodeficiency virus \", text)\n",
        "    text = re.sub(r\" UK \", \" england \", text)\n",
        "    text = re.sub(r\" ID \", \" identification \", text)\n",
        "    text = re.sub(r\" PM \", \" prime minister \", text)    \n",
        "    text = re.sub(r\" MA \", \" Massachusetts \", text)\n",
        "    text = re.sub(r\" WA \", \" washington \", text)\n",
        "    text = re.sub(r\" MN \", \" minnesota \", text)\n",
        "    text = re.sub(r\" NJ \", \" new jersey \", text)\n",
        "    text = re.sub(r\" NK \", \" north korea \", text)\n",
        "    text = re.sub(r\" NE \", \" nebraska \", text)\n",
        "    text = re.sub(r\" S.C. \", \" south carolina \", text)\n",
        "    text = re.sub(r\" NSA \", \" National Security Agency \", text)\n",
        "    text = re.sub(r\" NY \", \" new york \", text)\n",
        "    text = re.sub(r\" PRC \", \" china \", text)\n",
        "    text = re.sub(r\" CA \", \" canada \", text)\n",
        "    text = re.sub(r\" SA \", \" south africa \", text)\n",
        "    text = re.sub(r\" GA \", \" Georgia \", text)\n",
        "    text = re.sub(r\" UAE \", \" United Arab Emirates \", text)\n",
        "    text = re.sub(r\" UNICEF \", \" united nations children's fund  \", text)\n",
        "    text = re.sub(r\" U.N. \", \" united nations \", text)\n",
        "    text = re.sub(r\" UN \", \" united nations \", text)\n",
        "    text = re.sub(r\" US \", \" united states \", text)\n",
        "    text = re.sub(r\" U.S. \", \" united states \", text)\n",
        "    text = re.sub(r\" u.s. \", \" united states  \", text)\n",
        "    text = re.sub(r\" USA \", \" united states  \", text)\n",
        "    text = re.sub(r\" d.c. \", \" district of columbia  \", text)\n",
        "    text = re.sub(r\" N.D. \", \" no date \", text)\n",
        "    text = re.sub(r\" ICU \", \" university of south california \", text)\n",
        "    text = re.sub(r\" UTI \", \" urinary track infection \", text)\n",
        "    text = re.sub(r\" UIC \", \" illinois university \", text)\n",
        "    text = re.sub(r\" USDA \", \" unites states department of agriculture \", text)\n",
        "    text = re.sub(r\" PPE \", \" personal protective equipment \", text)    \n",
        "    text = re.sub(r\" PTSD \", \" post traumatic stress disorder \", text)    \n",
        "    text = re.sub(r'([0-9a-z]){25,} | (id[A-Z0-9]{11}) | (2020[0-9]{4}) | (\\d{12})_(\\d{1}) | (p\\d{2}g\\d{1}b) | (a\\d{7})', '', text)\n",
        "        \n",
        "    # Tokenize the string into word tokens\n",
        "    news_tokens = word_tokenize(text)\n",
        "\n",
        "    # Generate lemmas\n",
        "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in news_tokens]\n",
        "\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    alpha_lemmas = [lemma.lower() for lemma in lemmas \n",
        "                    if lemma.isalpha() and lemma not in stop_words_final]    \n",
        "\n",
        "    # return joined clenaed headlines strings  \n",
        "    return ' '.join(alpha_lemmas)\n",
        "\n",
        "# Function to merge the stock market and headlines data after preprocessing\n",
        "def join_news_and_stocks_data(stocks_file_path):\n",
        "\n",
        "    # read the stock market data from csv\n",
        "    stock_prices = pd.read_csv(stocks_file_path)\n",
        "\n",
        "    # Check for the data types and columns info of the data\n",
        "    print(\"Info of the stock market data: \\n\", stock_prices.info(verbose=True))\n",
        "    print(stock_prices.head(5))\n",
        "\n",
        "    # Remove the extra % string from the Change % column values and convert them to numeric values\n",
        "    stock_prices['Change %'] = stock_prices['Change %'].str.replace(r'%', '')\n",
        "    stock_prices[[\"Change %\"]] = stock_prices[[\"Change %\"]].apply(pd.to_numeric)\n",
        "\n",
        "    # Add a new column label with 1 for postive stock market trends and 0 for negative trends\n",
        "    stock_prices['Label'] = np.where(stock_prices['Change %'] > 0 , 1, 0)\n",
        "\n",
        "    # Delete the unnecessary columns\n",
        "    stock_prices = stock_prices.drop(['Change %','Low', 'Open', 'Volume', 'High', 'Price'], 1)\n",
        "\n",
        "    # Change the data string into datetime object\n",
        "    stock_prices['Date'] = pd.to_datetime(stock_prices['Date'])\n",
        "\n",
        "    # Check for the dimnesions, duplicate values, null values of stock market and headlines data\n",
        "    print(\"Modified stock market data\", stock_prices.head(5))\n",
        "    print(\"Dimensions of the modified stock market data\", stock_prices.shape)\n",
        "    print(\"Dimensions of the corona news data\", corona_news.shape)\n",
        "    print(\"Duplicate values present in stock market data: \", stock_prices['Date'].duplicated().any())\n",
        "    print(\"Duplicate values present in corona news data: \", corona_news['Date'].duplicated().any())\n",
        "    print(\"Number of missing values: \", stock_prices.isnull().sum())    \n",
        "\n",
        "    # merge the stock market and headlines data\n",
        "    final_data = pd.merge(corona_news, stock_prices, on=['Date'], how='inner')\n",
        "    return final_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0_MECiEohl1"
      },
      "source": [
        "# Classification using KNN \n",
        "def knn_classifier(X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    # set the hyper parameters\n",
        "    param_grid = {'n_neighbors' : np.arange(1, 50)}\n",
        "    knn = KNeighborsClassifier()\n",
        "\n",
        "    # Instantiate the GridSearchCV object\n",
        "    knn_cv = GridSearchCV(knn, param_grid, cv=10)\n",
        "\n",
        "    # fit the model\n",
        "    knn_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test set: y_pred\n",
        "    y_pred = knn_cv.predict(X_test)\n",
        "\n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of KNN is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of KNN is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Compute and print tuned parameters and score\n",
        "    print('Best parameters of KNN: {}'.format(knn_cv.best_params_))\n",
        "    print('Best score of KNN: {}'.format(knn_cv.best_score_))\n",
        "\n",
        "    # Plot the confusion matrix \n",
        "    fig = plot_confusion_matrix(knn_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "\n",
        "# Classification using SVM \n",
        "def svm_classifier(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Specify the hyperparameter space\n",
        "    parameters = {'C':[1, 10, 100],\n",
        "                  'gamma':[0.1, 0.01, 0.001]}\n",
        "\n",
        "    # Instantiate the GridSearchCV object\n",
        "    svm_cv = GridSearchCV(SVC(), parameters, cv=10)\n",
        "\n",
        "    # Fit to the training set\n",
        "    svm_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test set: y_pred\n",
        "    y_pred = svm_cv.predict(X_test)\n",
        "\n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of SVM is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of SVM is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Tuned Model Parameters of SVM: \", svm_cv.best_params_)\n",
        "    print('Best score of SVM: {}'.format(svm_cv.best_score_))\n",
        "\n",
        "    # Plot the confusion matrix \n",
        "    fig = plot_confusion_matrix(svm_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "    # Classification using Multinomial Naive Bayesian classifier \n",
        "\n",
        "def multinomialnb_classifier(X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    # Setup the parameters and distributions to sample from: param_dist\n",
        "    param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # Instantiate a multinomial naive bayes classifier\n",
        "    multinomialnb = MultinomialNB()\n",
        "\n",
        "    # Instantiate the GridSearchCV object\n",
        "    multinomialnb_cv = GridSearchCV(multinomialnb, param_dist, cv=10)\n",
        "\n",
        "    # Fit it to the training data\n",
        "    multinomialnb_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test set: y_pred\n",
        "    y_pred = multinomialnb_cv.predict(X_test)\n",
        " \n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of multinomial naive bayes classifier is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of multinomial naive bayes classifier is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Tuned multinomial naive bayes Parameters: {}\".format(multinomialnb_cv.best_params_))\n",
        "    print(\"Best score with multinomial naive bayes classifier is {}\".format(multinomialnb_cv.best_score_))\n",
        "    \n",
        "    # Plot the confusion matrix\n",
        "    fig = plot_confusion_matrix(multinomialnb_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "\n",
        "# Classification using Stochastic gradient descent classifier \n",
        "def sgd_classifier(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Setup the hyperparameter grid\n",
        "    param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'penalty':['l2'], 'loss': ['hinge']}\n",
        "\n",
        "    # Instantiate a Stochastic Gradient Descent classifier\n",
        "    sgd = SGDClassifier(max_iter=500)\n",
        "\n",
        "    # Instantiate the GridSearchCV object\n",
        "    sgd_cv = GridSearchCV(sgd, param_grid, cv=10)\n",
        "\n",
        "    # Fit the classifier to the training data\n",
        "    sgd_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test data: y_pred\n",
        "    y_pred = sgd_cv.predict(X_test)\n",
        "    \n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of Stochastic Gradient Descent classifier is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of Stochastic Gradient Descent classifier is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Tuned Model Parameters of Stochastic Gradient Descent classifier: \", sgd_cv.best_params_)\n",
        "    print('Best score of Stochastic Gradient Descent classifier: {}'.format(sgd_cv.best_score_))\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    fig = plot_confusion_matrix(sgd_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "\n",
        "# Classification using Random forest classifier \n",
        "def random_forest_classifier(X_train, X_test, y_train, y_test):\n",
        "    # Setup the parameters and distributions to sample from: param_dist\n",
        "    param_dist = {\"max_depth\": [3, None],\n",
        "                \"max_features\": randint(1, 9),\n",
        "                \"min_samples_leaf\": randint(1, 9),\n",
        "                \"n_estimators\": randint(1, 100),\n",
        "                \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "    # Instantiate a Random forest classifier\n",
        "    random = RandomForestClassifier()\n",
        "\n",
        "    # Instantiate the RandomizedSearchCV object\n",
        "    random_cv = RandomizedSearchCV(random, param_dist, cv=10)\n",
        "\n",
        "    # Fit it to the training data\n",
        "    random_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test set: y_pred\n",
        "    y_pred = random_cv.predict(X_test)\n",
        " \n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of random forest classifier is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of random forest classifier is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Tuned random forest classifier Parameters: {}\".format(random_cv.best_params_))\n",
        "    print(\"Best score random forest classifier is {}\".format(random_cv.best_score_))\n",
        "    \n",
        "    # Plot the confusion matrix\n",
        "    fig = plot_confusion_matrix(random_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "\n",
        "# Classification using Adaboost classifier \n",
        "def adaboost_classifier(X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    # Instantiate classifier\n",
        "    adaboost = AdaBoostClassifier()\n",
        "\n",
        "    # Specify the hyperparameter space\n",
        "    parameters = {'n_estimators' : np.arange(1, 50)}\n",
        "\n",
        "    # Instantiate the GridSearchCV object\n",
        "    ada_cv = GridSearchCV(adaboost, parameters, cv=10)\n",
        "\n",
        "    # Fit to the training set\n",
        "    ada_cv.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels of the test set: y_pred\n",
        "    y_pred = ada_cv.predict(X_test)\n",
        "\n",
        "    # Compute and print metrics\n",
        "    print(\"Classification report of AdaBoost is: \\n\", classification_report(y_test, y_pred))\n",
        "    #print(\"Confusion Matrix of AdaBoost is: \\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Tuned Model Parameters of AdaBoost: \", ada_cv.best_params_)\n",
        "    print(\"Best score of AdaBoost is {}\".format(ada_cv.best_score_))    \n",
        "\n",
        "    # Plot the confusion matrix \n",
        "    fig = plot_confusion_matrix(ada_cv, X_test, y_test, \n",
        "                            cmap=plt.cm.Blues)\n",
        "    fig.ax_.set_title(\"Confusion Matrix plot\")\n",
        "    print(fig.confusion_matrix)\n",
        "    plt.show()\n",
        "\n",
        "# Function to vectorize with 3 types of vectorizers and classify with 6 types of classifiers the data\n",
        "def vectorize_and_classify(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    X_train_org = X_train\n",
        "    X_test_org = X_test\n",
        "    \n",
        "    # Generating Bag-of-Words\n",
        "    vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
        "\n",
        "    # Generating bigrams\n",
        "    vectorizer_ng2 = CountVectorizer(ngram_range=(2,2))\n",
        "\n",
        "    # Generating trigrams\n",
        "    #vectorizer_ng3 = CountVectorizer(ngram_range=(3,3))\n",
        "\n",
        "    # Generating TF-IDF vectors\n",
        "    vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "    # create a list of vectorizers\n",
        "    vectorizer_list = [vectorizer_ng1, vectorizer_ng2, vectorizer_tfidf]\n",
        "    \n",
        "    for vectorizer in vectorizer_list:\n",
        "        print(\".........Vectorizing the headlines using vectorizer..........\")\n",
        "        # fit and transform the data with the vectorizer\n",
        "        X_train = vectorizer.fit_transform(X_train)\n",
        "        X_test = vectorizer.transform(X_test)\n",
        "\n",
        "        # Classify data using multiple classifiers\n",
        "        knn_classifier(X_train, X_test, y_train, y_test)\n",
        "        svm_classifier(X_train, X_test, y_train, y_test)\n",
        "        multinomialnb_classifier(X_train, X_test, y_train, y_test)\n",
        "        sgd_classifier(X_train, X_test, y_train, y_test)\n",
        "        random_forest_classifier(X_train, X_test, y_train, y_test)\n",
        "        adaboost_classifier(X_train, X_test, y_train, y_test)\n",
        "\n",
        "        # revert back the test and training sets before next vectorization\n",
        "        X_train = X_train_org\n",
        "        X_test = X_test_org\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCRlzxgT0In7"
      },
      "source": [
        "# Function to classify data using word embedding LSTM\n",
        "def word_embedding_LSTM(X_train, X_test, y_train, y_test):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(model_data['News'], model_data['Label'], test_size=0.2, random_state=42) \n",
        "\n",
        "    # fit the preprocessed data with tokenizer\n",
        "    tokenizer = Tokenizer(num_words=30000)\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "    # get the vocabulary size       \n",
        "    vocabulary_size = (len(tokenizer.word_index) + 1)\n",
        "    print('Number of words: ', vocabulary_size)\n",
        "\n",
        "    # convert text to sequences\n",
        "    X_train_seqs = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seqs = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    max_length = 500\n",
        "    print('Input length: ', max_length)\n",
        "\n",
        "    # pad the sequences\n",
        "    X_train_pad = pad_sequences(X_train_seqs, maxlen=max_length, padding='post')\n",
        "    X_test_pad = pad_sequences(X_test_seqs, maxlen=max_length, padding='post')\n",
        "\n",
        "    embedding_vector_length = 64\n",
        "\n",
        "    # build the model, fit it on data and evaluate the results\n",
        "    print('Build LSTM model with word embedding...')\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, embedding_vector_length, input_length=max_length))\n",
        "    model.add(LSTM(50, dropout=0.2)) \n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    print('.......Train the data using LSTM.......')\n",
        "    model.fit(X_train_pad, y_train, batch_size=32, epochs=50, verbose=0, validation_data=(X_test_pad, y_test))\n",
        "    score, acc = model.evaluate(X_test_pad, y_test, batch_size=32)\n",
        "    print('Test score:', score)\n",
        "    print('Test accuracy:', acc)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m45EH-0dPjjD"
      },
      "source": [
        "# Function to get user input\n",
        "def take_user_input():\n",
        "    numbers_list = []\n",
        "    countries_dict = {}\n",
        "\n",
        "    # get the unique values of countries in a list\n",
        "    country_list = final_data.Country.unique().tolist()\n",
        "\n",
        "    # make a list of numbers\n",
        "    for i in range(1, 15):\n",
        "        numbers_list.append(i)\n",
        "\n",
        "    # combine numbers list and countries list into a dictionary\n",
        "    for key in numbers_list:\n",
        "        for value in country_list:\n",
        "            countries_dict[key] = value \n",
        "            country_list.remove(value)\n",
        "            break\n",
        "\n",
        "    # print the options to user\n",
        "    print(\"Below is the list of countries for which you can classify stock market trends using corona virus headlines: \")\n",
        "    for key, value in countries_dict.items():\n",
        "        print(key, value, sep=' -> ')\n",
        "    \n",
        "    option = int(input(\"Enter an option: \"))\n",
        "\n",
        "    # get the country value for user option\n",
        "    country = countries_dict.get(option)\n",
        "    \n",
        "    print(\"The country entered is: \", country)\n",
        "    return country\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOOqB14T6Na-",
        "outputId": "8b50e511-703e-44d0-c5ff-4a56770e92ca"
      },
      "source": [
        " if __name__ == \"__main__\":\n",
        "\n",
        "    # initialize the variables\n",
        "    news_file_path = 'corona_news.csv'\n",
        "    stocks_file_path = 'stock_prices_merged.csv'\n",
        "\n",
        "    # call function and get data with combined headlines strings\n",
        "    #corona_news = combine_news_strings(news_file_path)\n",
        "\n",
        "    # make a long string of all the headlines to find abbreviations\n",
        "    #combined_news = ' '.join(corona_news.News)\n",
        "\n",
        "    # Call function to find abbreviations\n",
        "    #abbr_array = search_abbreviation(combined_news)    \n",
        "    #np.set_printoptions(threshold=sys.maxsize) #for long output of abbreviations list\n",
        "    print(\"Abbreviations present in the headlines are: \\n\", abbr_array)\n",
        "\n",
        "    # do pre-processing of the headlines\n",
        "    corona_news['News'] = corona_news['News'].apply(tokenize_and_preprocess_text)\n",
        "    print(\".............Text preprocessing is done..............\")\n",
        "\n",
        "    print(\"preprocessed headlines data: \\n\", corona_news.head(5))\n",
        "\n",
        "    # merge headlines and stock prices data\n",
        "    final_data = join_news_and_stocks_data(stocks_file_path)\n",
        "\n",
        "    print(\"final data with headlines and stock market labels: \\n\", final_data.head(15)) \n",
        "    print(\"final data null values: \", final_data.isnull().sum())\n",
        "    print(\"Aummary statistics of final data: \\n\", final_data.describe())\n",
        "\n",
        "    # get user input\n",
        "    country = take_user_input()\n",
        "\n",
        "    # validate the user input\n",
        "    while country == None:\n",
        "        print(\"Please enter a valid option\")\n",
        "        country = take_user_input()\n",
        "\n",
        "    print(\".................Classification started..................\")\n",
        "\n",
        "    # filter the data countrywise\n",
        "    model_data = final_data[final_data['Country'] == country]\n",
        "    print(\"Country-wise data: \\n\", model_data.head(5))\n",
        "    print(\"Dimensions of country-wise data: \", model_data.shape)\n",
        "    model_data = model_data.reset_index(drop=True)\n",
        "    print(\"======================== Classifying stock market labels for\", country, \"============================\")\n",
        "\n",
        "    # Splitting the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(model_data['News'], model_data['Label'], test_size=0.2, random_state=42)        \n",
        "\n",
        "    # vectorize and classify the data\n",
        "    vectorize_and_classify(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # vectorize data with word embedding and classify with LSTM\n",
        "    word_embedding_LSTM(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abbreviations present in the headlines are: \n",
            " ['COVID' 'CT' 'UKKBN' ... 'PDM' 'NSSF' 'SUSPEND']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL_3M6Y4Hw8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}